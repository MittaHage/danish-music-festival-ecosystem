{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2dca0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os, re, time, urllib.parse, urllib.request, gzip, json\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "import community.community_louvain as community_louvain\n",
    "import zipfile\n",
    "import io\n",
    "from itertools import combinations\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd34e8",
   "metadata": {},
   "source": [
    "## Analysing Communities\n",
    "In order to analyse communities we would have to project to bipartite graph to a graph only consisting of the festivals, and they will be connected by weighted edges to other festivals. \n",
    "\n",
    "The communities are then created using the Louvin algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9320c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use the raw URL, not the \"blob\" link\n",
    "url = \"https://raw.githubusercontent.com/MittaHage/danish-music-festival-ecosystem/main/festival_network_attributes.json\"\n",
    "\n",
    "# Download and decode\n",
    "response = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "\n",
    "# Load JSON\n",
    "data = json.loads(response)\n",
    "\n",
    "# Convert to NetworkX graph\n",
    "B = nx.node_link_graph(data, edges=\"links\")\n",
    "\n",
    "festival_year_nodes = [n for n, d in B.nodes(data=True) if d.get(\"bipartite\") == \"festival_year\"]\n",
    "if not festival_year_nodes:\n",
    "    raise ValueError(\"No nodes found with bipartite=='festival_year'\")\n",
    "\n",
    "nbrs = {u: set(B.neighbors(u)) for u in festival_year_nodes}\n",
    "\n",
    "G = nx.Graph()\n",
    "for u in festival_year_nodes:\n",
    "    G.add_node(u, **B.nodes[u])\n",
    "\n",
    "for u, v in combinations(festival_year_nodes, 2):\n",
    "    inter = len(nbrs[u] & nbrs[v])\n",
    "    if inter == 0:\n",
    "        continue\n",
    "    union = len(nbrs[u] | nbrs[v])\n",
    "    G.add_edge(u, v, weight=inter / union)\n",
    "\n",
    "communities = nx.algorithms.community.louvain_communities(G, weight=\"weight\", resolution=1.0, seed=0)\n",
    "communities = sorted(communities, key=len, reverse=True)\n",
    "node_to_comm = {node: cid for cid, comm in enumerate(communities) for node in comm}\n",
    "\n",
    "print(\"Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges(), \"Communities:\", len(communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb5feb",
   "metadata": {},
   "source": [
    "### Visualizing the communities\n",
    "In order to get a further understanding of the communities and the festivals placement within, a visualization is made where it is shown which community each festival is clustered in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b628e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Creating community plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "FESTIVAL_LABELS = {\n",
    "    \"roskilde\": \"Roskilde Festival\",\n",
    "    \"smukfest\": \"Smukfest\",\n",
    "    \"copenhell\": \"Copenhell\",\n",
    "    \"copenhagen\": \"Copenhagen Jazz Festival\",\n",
    "    \"groen\": \"Gr√∏n Koncert\",\n",
    "    \"jelling\": \"Jelling Musikfestival\",\n",
    "    \"nibe\": \"Nibe Festival\",\n",
    "    \"rock\": \"Rock Under Broen\",\n",
    "    \"skive\": \"Skive Festival\",\n",
    "    \"vig\": \"Vig Festival\",\n",
    "    \"northside\": \"NorthSide\",\n",
    "    \"tinderbox\": \"Tinderbox\",\n",
    "}\n",
    "\n",
    "def plot_festival_timeline_top_communities(B, node_to_comm, top_k=7, festival_labels=None):\n",
    "    festival_labels = festival_labels or {}\n",
    "\n",
    "    comm_sizes = Counter()\n",
    "    valid = []\n",
    "    for node, cid in node_to_comm.items():\n",
    "        if node not in B:\n",
    "            continue\n",
    "        d = B.nodes[node]\n",
    "        if d.get(\"bipartite\") != \"festival_year\":\n",
    "            continue\n",
    "        fest = d.get(\"festival\")\n",
    "        year = d.get(\"year\")\n",
    "        if fest is None or year is None:\n",
    "            continue\n",
    "        try:\n",
    "            int(year)\n",
    "        except:\n",
    "            continue\n",
    "        valid.append(node)\n",
    "        comm_sizes[cid] += 1\n",
    "\n",
    "    non_singletons = [(cid, sz) for cid, sz in comm_sizes.items() if sz > 1]\n",
    "    non_singletons.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_comms = [cid for cid, _ in non_singletons[:top_k]]\n",
    "    top_set = set(top_comms)\n",
    "\n",
    "    by_fest = defaultdict(list)\n",
    "    for node in valid:\n",
    "        cid = node_to_comm[node]\n",
    "        if cid not in top_set:\n",
    "            continue\n",
    "        d = B.nodes[node]\n",
    "        by_fest[d[\"festival\"]].append((int(d[\"year\"]), cid))\n",
    "\n",
    "    if not by_fest:\n",
    "        print(\"Nothing to plot: no nodes belong to the selected communities.\")\n",
    "        return\n",
    "\n",
    "    for fest in by_fest:\n",
    "        by_fest[fest].sort()\n",
    "\n",
    "    # Sort by first year, and reverse order\n",
    "    festivals_raw = sorted(by_fest.keys(), key=lambda f: by_fest[f][0][0])[::-1]\n",
    "    festivals_display = [festival_labels.get(f, f) for f in festivals_raw]\n",
    "\n",
    "    cid_to_idx = {cid: i for i, cid in enumerate(top_comms)}\n",
    "    cmap = plt.get_cmap(\"tab10\" if len(top_comms) <= 10 else \"tab20\", len(top_comms))\n",
    "\n",
    "    xs, ys, cols = [], [], []\n",
    "    for row, fest in enumerate(festivals_raw):\n",
    "        for year, cid in by_fest[fest]:\n",
    "            xs.append(year)\n",
    "            ys.append(row)\n",
    "            cols.append(cmap(cid_to_idx[cid]))\n",
    "\n",
    "    fig_h = max(5, 0.55 * len(festivals_raw))\n",
    "    plt.figure(figsize=(14, fig_h))\n",
    "    plt.scatter(xs, ys, c=cols, s=120, alpha=0.95, edgecolors=\"black\", linewidths=0.7)\n",
    "\n",
    "    plt.yticks(range(len(festivals_raw)), festivals_display)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.grid(axis=\"x\", alpha=0.25)\n",
    "    plt.xlim(min(xs) - 1, max(xs) + 1)\n",
    "\n",
    "    handles = []\n",
    "    for cid in top_comms:\n",
    "        i = cid_to_idx[cid]\n",
    "        handles.append(\n",
    "            Line2D([0], [0], marker=\"o\", linestyle=\"\",\n",
    "                   markersize=9, markerfacecolor=cmap(i),\n",
    "                   markeredgecolor=\"black\", markeredgewidth=0.8,\n",
    "                   label=f\"comm {cid+1} (n={comm_sizes[cid]})\")\n",
    "        )\n",
    "\n",
    "    plt.legend(handles=handles, title=\"Top communities\", loc=\"lower left\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_festival_timeline_top_communities(B, node_to_comm, top_k=7, festival_labels=FESTIVAL_LABELS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82580c75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb8b93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Communities list for modularity\n",
    "comm_to_nodes = defaultdict(set)\n",
    "for node, cid in node_to_comm.items():\n",
    "    comm_to_nodes[cid].add(node)\n",
    "communities_list = list(comm_to_nodes.values())\n",
    "\n",
    "# Overall modularity\n",
    "mod = nx.algorithms.community.quality.modularity(G, communities_list, weight=\"weight\")\n",
    "print(\"Overall modularity (partition on projected graph):\", round(mod, 4))\n",
    "\n",
    "def comm_internal_weight(G, S):\n",
    "    total = 0.0\n",
    "    for u in S:\n",
    "        for v, attrs in G[u].items():\n",
    "            if v in S and u < v:\n",
    "                total += attrs.get(\"weight\", 1.0)\n",
    "    return total\n",
    "\n",
    "def comm_cut_weight(G, S):\n",
    "    total = 0.0\n",
    "    for u in S:\n",
    "        for v, attrs in G[u].items():\n",
    "            if v not in S:\n",
    "                total += attrs.get(\"weight\", 1.0)\n",
    "    return total\n",
    "\n",
    "def weighted_degree(G, u):\n",
    "    return sum(attrs.get(\"weight\", 1.0) for _, attrs in G[u].items())\n",
    "\n",
    "def comm_volume(G, S):\n",
    "    return sum(weighted_degree(G, u) for u in S)\n",
    "\n",
    "def to_percent_dict(d):\n",
    "    total = sum(d.values())\n",
    "    if total <= 0:\n",
    "        return {}\n",
    "    return {k: 100.0 * v / total for k, v in d.items()}\n",
    "\n",
    "comm_genres = defaultdict(lambda: defaultdict(float)) \n",
    "comm_pop = defaultdict(list)\n",
    "comm_years = defaultdict(list)\n",
    "comm_fest = defaultdict(Counter)\n",
    "\n",
    "missing = 0\n",
    "\n",
    "for fy_node, cid in node_to_comm.items():\n",
    "    if fy_node not in B:\n",
    "        missing += 1\n",
    "        continue\n",
    "\n",
    "    d = B.nodes[fy_node]\n",
    "    y = d.get(\"year\")\n",
    "    if y is not None:\n",
    "        try:\n",
    "            comm_years[cid].append(int(y))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    f = d.get(\"festival\")\n",
    "    if f is not None:\n",
    "        comm_fest[cid][f] += 1\n",
    "\n",
    "    for a in B.neighbors(fy_node):\n",
    "        if B.nodes[a].get(\"bipartite\") != \"artist\":\n",
    "            continue\n",
    "\n",
    "        genres = B.nodes[a].get(\"genres\") or []\n",
    "        # Fractional counting: total contribution per artist = 1 split across genres\n",
    "        if genres:\n",
    "            w = 1.0 / len(genres)\n",
    "            for g in genres:\n",
    "                if g:  # keep original names, just skip empty\n",
    "                    comm_genres[cid][g] += w\n",
    "\n",
    "        pop = B.nodes[a].get(\"popularity\")\n",
    "        if isinstance(pop, (int, float)):\n",
    "            comm_pop[cid].append(pop)\n",
    "\n",
    "print(\"Skipped festival-year nodes not in B:\", missing)\n",
    "\n",
    "# Print descriptive blocks\n",
    "all_cids = sorted(comm_to_nodes.keys(), key=lambda c: len(comm_to_nodes[c]), reverse=True)\n",
    "\n",
    "for cid in all_cids:\n",
    "    S = comm_to_nodes[cid]\n",
    "\n",
    "    n_nodes = len(S)\n",
    "    w_int = comm_internal_weight(G, S)\n",
    "    w_cut = comm_cut_weight(G, S)\n",
    "    volS = comm_volume(G, S)\n",
    "    volNotS = comm_volume(G, set(G.nodes()) - set(S))\n",
    "    denom = min(volS, volNotS) if min(volS, volNotS) > 0 else None\n",
    "    conductance = (w_cut / denom) if denom else None\n",
    "\n",
    "    years = comm_years.get(cid, [])\n",
    "    year_span = (min(years), max(years)) if years else None\n",
    "    top_fests = comm_fest[cid].most_common(5)\n",
    "\n",
    "    pops = comm_pop.get(cid, [])\n",
    "    mean_pop = float(np.mean(pops)) if pops else None\n",
    "    med_pop = float(np.median(pops)) if pops else None\n",
    "\n",
    "    # ORIGINAL genre names, fractional, normalized to percentages\n",
    "    raw_pct = to_percent_dict(comm_genres[cid])\n",
    "    top_raw = sorted(raw_pct.items(), key=lambda kv: kv[1], reverse=True)[:12]\n",
    "\n",
    "    print(f\"\\n=== Community {cid} ===\")\n",
    "    print(f\"festival-year nodes: {n_nodes}\")\n",
    "    print(f\"year span: {year_span}\" if year_span else \"year span: n/a\")\n",
    "    print(\"top festivals:\", top_fests)\n",
    "    print(f\"internal edge weight: {w_int:.3f} | cut weight: {w_cut:.3f}\")\n",
    "    print(\"conductance (lower=more separated):\", round(conductance, 4) if conductance is not None else \"n/a\")\n",
    "    print(\"artist popularity mean/median:\", (round(mean_pop, 2), round(med_pop, 2)) if pops else \"n/a\")\n",
    "    print(\"original genre % (fractional, top):\", [(g, round(p, 2)) for g, p in top_raw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd52d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# -------------------------\n",
    "# CONTROL FESTIVAL NAMES HERE (legend labels)\n",
    "# keys must match node attribute: d[\"festival\"]\n",
    "# -------------------------\n",
    "FESTIVAL_LABELS = {\n",
    "    \"roskilde\": \"Roskilde Festival\",\n",
    "    \"smukfest\": \"Smukfest\",\n",
    "    \"copenhell\": \"Copenhell\",\n",
    "    \"copenhagen\": \"Copenhagen Jazz Festival\",\n",
    "    \"groen\": \"Gr√∏n Koncert\",\n",
    "    \"jelling\": \"Jelling Musikfestival\",\n",
    "    \"nibe\": \"Nibe Festival\",\n",
    "    \"rock\": \"Rock Under Broen\",\n",
    "    \"skive\": \"Skive Festival\",\n",
    "    \"vig\": \"Vig Festival\",\n",
    "    \"northside\": \"NorthSide\",\n",
    "    \"tinderbox\": \"Tinderbox\",\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load bipartite graph\n",
    "# -------------------------\n",
    "with open(\"festival_network_attributes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    B = nx.node_link_graph(json.load(f), edges=\"links\")\n",
    "\n",
    "festival_nodes = [n for n, d in B.nodes(data=True) if d.get(\"bipartite\") == \"festival_year\"]\n",
    "\n",
    "# -------------------------\n",
    "# 2) Project with Jaccard weights\n",
    "# -------------------------\n",
    "nbrs = {u: set(B.neighbors(u)) for u in festival_nodes}\n",
    "\n",
    "G = nx.Graph()\n",
    "for u in festival_nodes:\n",
    "    G.add_node(u, **B.nodes[u])\n",
    "\n",
    "for u, v in combinations(festival_nodes, 2):\n",
    "    inter = len(nbrs[u] & nbrs[v])\n",
    "    if inter == 0:\n",
    "        continue\n",
    "    union = len(nbrs[u] | nbrs[v])\n",
    "    w = inter / union\n",
    "    G.add_edge(u, v, weight=w)\n",
    "\n",
    "print(\"Projected graph:\", G.number_of_nodes(), \"nodes,\", G.number_of_edges(), \"edges\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Backbone: kNN edges per node\n",
    "# -------------------------\n",
    "def knn_backbone(G, k=6, weight=\"weight\", union=True, min_weight=0.0):\n",
    "    keep = set()\n",
    "    for u in G.nodes():\n",
    "        edges = []\n",
    "        for v, d in G[u].items():\n",
    "            w = d.get(weight, 1.0)\n",
    "            if w >= min_weight:\n",
    "                edges.append((w, u, v))\n",
    "        edges.sort(reverse=True)\n",
    "        for w, a, b in edges[:k]:\n",
    "            keep.add((a, b) if a < b else (b, a))\n",
    "\n",
    "    H = nx.Graph()\n",
    "    H.add_nodes_from(G.nodes(data=True))\n",
    "\n",
    "    if union:\n",
    "        for a, b in keep:\n",
    "            if G.has_edge(a, b):\n",
    "                H.add_edge(a, b, **G[a][b])\n",
    "    else:\n",
    "        chosen = defaultdict(set)\n",
    "        for a, b in keep:\n",
    "            chosen[a].add(b)\n",
    "            chosen[b].add(a)\n",
    "        for a, b in keep:\n",
    "            if (b in chosen[a]) and (a in chosen[b]) and G.has_edge(a, b):\n",
    "                H.add_edge(a, b, **G[a][b])\n",
    "\n",
    "    isolates = [n for n in H.nodes() if H.degree(n) == 0]\n",
    "    H.remove_nodes_from(isolates)\n",
    "    return H\n",
    "\n",
    "H = knn_backbone(G, k=6, min_weight=0.03, union=True)\n",
    "print(\"Backbone:\", H.number_of_nodes(), \"nodes,\", H.number_of_edges(), \"edges\")\n",
    "\n",
    "# largest connected component (optional; makes one clean figure)\n",
    "if H.number_of_nodes() > 0:\n",
    "    cc = max(nx.connected_components(H), key=len)\n",
    "    H = H.subgraph(cc).copy()\n",
    "\n",
    "# -------------------------\n",
    "# 4) Plot with legend (no labels on nodes)\n",
    "# -------------------------\n",
    "def plot_projected_backbone_with_legend(H, seed=42):\n",
    "    pos = nx.spring_layout(H, seed=seed, weight=\"weight\")\n",
    "\n",
    "    festivals = sorted({H.nodes[n].get(\"festival\", \"unknown\") for n in H.nodes()})\n",
    "    cmap = plt.get_cmap(\"tab20\", max(1, len(festivals)))\n",
    "    fest_color = {f: cmap(i) for i, f in enumerate(festivals)}\n",
    "\n",
    "    node_colors = [fest_color.get(H.nodes[n].get(\"festival\", \"unknown\")) for n in H.nodes()]\n",
    "\n",
    "    # node size = weighted degree (strength)\n",
    "    strength = {n: sum(H[n][nbr].get(\"weight\", 1.0) for nbr in H.neighbors(n)) for n in H.nodes()}\n",
    "    svals = list(strength.values()) or [1.0]\n",
    "    smin, smax = min(svals), max(svals)\n",
    "    def scale(x):\n",
    "        if smax == smin:\n",
    "            return 90\n",
    "        return 60 + 260 * (x - smin) / (smax - smin)\n",
    "    node_sizes = [scale(strength[n]) for n in H.nodes()]\n",
    "\n",
    "    # edges: width by weight\n",
    "    wvals = [d.get(\"weight\", 1.0) for _, _, d in H.edges(data=True)] or [1.0]\n",
    "    wmin, wmax = min(wvals), max(wvals)\n",
    "    def escale(w):\n",
    "        if wmax == wmin:\n",
    "            return 1.0\n",
    "        return 0.4 + 4.0 * (w - wmin) / (wmax - wmin)\n",
    "    edge_widths = [escale(d.get(\"weight\", 1.0)) for _, _, d in H.edges(data=True)]\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.18, width=edge_widths)\n",
    "    nx.draw_networkx_nodes(\n",
    "        H, pos,\n",
    "        node_color=node_colors,\n",
    "        node_size=node_sizes,\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=0.3,\n",
    "        alpha=0.95\n",
    "    )\n",
    "\n",
    "    # Legend handles (festival -> color)\n",
    "    handles = []\n",
    "    for f in festivals:\n",
    "        label = FESTIVAL_LABELS.get(f, f)  # <-- control names via FESTIVAL_LABELS\n",
    "        handles.append(Line2D(\n",
    "            [0], [0], marker='o', linestyle='',\n",
    "            markerfacecolor=fest_color[f], markeredgecolor='black',\n",
    "            markersize=9, label=label\n",
    "        ))\n",
    "\n",
    "    plt.legend(\n",
    "        handles=handles,\n",
    "        title=\"Festival\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=True\n",
    "    )\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_projected_backbone_with_legend(H, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898b83f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Map node -> community id\n",
    "comms = sorted(communities, key=len, reverse=True)\n",
    "cid = {n: i for i, C in enumerate(comms) for n in C}\n",
    "TOP_K = 6  # highlight top-K communities\n",
    "\n",
    "# Node sizes based on degree\n",
    "deg = np.array([G.degree(n) for n in G.nodes()], float)\n",
    "deg = (deg - deg.min()) / (deg.max() - deg.min() + 1e-9)\n",
    "sizes = 100 + deg * (2000 - 100)\n",
    "\n",
    "# Layout\n",
    "pos = nx.forceatlas2_layout(G, max_iter=500)\n",
    "\n",
    "# Colors by community\n",
    "import matplotlib.cm as cm\n",
    "cmap = cm.get_cmap(\"tab10\", TOP_K)\n",
    "colors = [cmap(cid[n]) if cid[n] < TOP_K else (0.85, 0.85, 0.85, 0.7) for n in G.nodes()]\n",
    "\n",
    "# Separate artists vs festivals\n",
    "artist_nodes = [n for n in G.nodes() if G.nodes[n].get(\"bipartite\") == \"artist\"]\n",
    "festival_nodes = [n for n in G.nodes() if G.nodes[n].get(\"bipartite\") == \"festival_year\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.25, width=0.4, ax=ax)\n",
    "\n",
    "# Draw artists (circles)\n",
    "nx.draw_networkx_nodes(G, pos,\n",
    "                       nodelist=artist_nodes,\n",
    "                       node_size=[sizes[list(G.nodes()).index(n)] for n in artist_nodes],\n",
    "                       node_color=[colors[list(G.nodes()).index(n)] for n in artist_nodes],\n",
    "                       node_shape=\"o\", ax=ax)\n",
    "\n",
    "# Draw festivals (squares)\n",
    "nx.draw_networkx_nodes(G, pos,\n",
    "                       nodelist=festival_nodes,\n",
    "                       node_size=[sizes[list(G.nodes()).index(n)] for n in festival_nodes],\n",
    "                       node_color=[colors[list(G.nodes()).index(n)] for n in festival_nodes],\n",
    "                       node_shape=\"s\", ax=ax)\n",
    "\n",
    "ax.set_title(f\"ForceAtlas2 ‚Ä¢ Louvain Communities ‚Ä¢ M = {M_louvain:.3f}\")\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfb777",
   "metadata": {},
   "source": [
    "## NLP - natural language processing\n",
    "When performing analysis through NLP wikipedia pages, are searched for in order to obtain some text material to analyse, using different tools tought trhoughout the course, in order to further examine our research question. \n",
    "\n",
    "The choice to perform text analysis on the available wikipedia pages for the artists whom have performed at any of the festivals examined in our research, is made since even though not every artist has a page, we were not able to find a better suited website, and this was the one where most artists were represented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49984e8f",
   "metadata": {},
   "source": [
    "#### Retrieving the graphs\n",
    "When doing the natural language processing we would have to retrieve the graph where the wikitext and sentiment value are added as attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393ecab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Raw URL to the zip file\n",
    "zip_url = \"https://raw.githubusercontent.com/MittaHage/danish-music-festival-ecosystem/main/festival_graph_newWiki.zip\"\n",
    "\n",
    "# Download zip into memory\n",
    "response = urllib.request.urlopen(zip_url).read()\n",
    "\n",
    "# Open zip from memory\n",
    "with zipfile.ZipFile(io.BytesIO(response)) as z:\n",
    "    print(\"Files inside zip:\", z.namelist())  # check contents\n",
    "    \n",
    "    # Assuming the JSON file is inside the zip\n",
    "    with z.open(\"festival_graph_newWiki.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "# Convert to NetworkX graph\n",
    "GWiki = json_graph.node_link_graph(data)\n",
    "print(\"Graph has\", GWiki.number_of_nodes(), \"nodes and\", GWiki.number_of_edges(), \"edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612b610",
   "metadata": {},
   "source": [
    "#### Cleaning the data\n",
    "To conduct meaningful text analysis, it is necessary to preprocess and clean the textual data. When applying TF‚ÄìIDF, the presence of multiple languages would distort the results, as terms cannot be consistently compared across linguistic boundaries. Therefore, Danish wikitexts are removed at this stage. Since sentiment values have already been calculated for the corresponding nodes, their wikitext entries are simply set to None, ensuring that the sentiment information is preserved while avoiding interference in the TF‚ÄìIDF analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcac992",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    if not isinstance(txt, str): return \"\"\n",
    "    txt = re.sub(r\"\\{\\{.*?\\}\\}\", \" \", txt, flags=re.S)  # remove templates\n",
    "    txt = re.sub(r\"<ref.*?>.*?</ref>\", \" \", txt, flags=re.S)  # remove refs\n",
    "    txt = re.sub(r\"==.*?==\", \" \", txt)  # remove section headers\n",
    "    txt = re.sub(r\"\\[\\[|\\]\\]|\\{|}|==|''+\", \" \", txt)\n",
    "    txt = re.sub(r\"http\\S+\", \" \", txt)\n",
    "    txt = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", txt)\n",
    "    return re.sub(r\"\\s+\", \" \", txt).strip().lower()\n",
    "\n",
    "# ---------- pick top-K  communities ----------\n",
    "# communities already computed: `communities`\n",
    "comms_sorted = sorted(communities, key=len, reverse=True)\n",
    "TOPK_COMMS = 6\n",
    "top_comm_ids = list(range(min(TOPK_COMMS, len(comms_sorted))))\n",
    "node2comm = {n: i for i, C in enumerate(comms_sorted) for n in C}\n",
    "\n",
    "# -------------- Translate the danish nodes --------------\n",
    "# Loop through all nodes in the graph\n",
    "for node, data in B.nodes(data=True):\n",
    "    # Check if node has wikitext and is in Danish\n",
    "    if data.get(\"wikitext\") and data.get(\"wiki_language\") == \"da\":\n",
    "            # Translate to English\n",
    "        B.nodes[node][\"wikitext\"] = None            \n",
    "        print(f\"Deleted danish node {node}\")  # preview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b53dc",
   "metadata": {},
   "source": [
    "### Calculating sentiment for communities\n",
    "In order to calculate the sentiment for the identified communities, the sentiment values are found in graph GWiki, for those nodes where the artist performed at the festival in the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b341d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --------------- Create Sentiment Analysis ---------------\n",
    "# Calculate the sentiment for the 10 largest communities\n",
    "\n",
    "# Step 1: Select the 7 largest communities - as these are the communities for which we made TF.IDF analysis\n",
    "sorted_communities = sorted(communities, key=len, reverse=True)\n",
    "top_communities = sorted_communities[:6]\n",
    "\n",
    "community_info = []\n",
    "\n",
    "for i, community in enumerate(top_communities):\n",
    "    # Find festival-nodes in community (from G)\n",
    "    festivals_in_comm = [n for n in community if G.nodes[n].get(\"bipartite\") == \"festival_year\"]\n",
    "\n",
    "    # Find all artists connected to those festivals in B(bipartite graph) \n",
    "    artist_neighbors = set()\n",
    "    for f in festivals_in_comm:\n",
    "        artist_neighbors.update([\n",
    "            nbr for nbr in B.neighbors(f)\n",
    "            if B.nodes[nbr].get(\"bipartite\") == \"artist\"\n",
    "        ])\n",
    "\n",
    "    # Calculate Average sentiment for artists, using GWiki since this is the graph with sentimentvalue\n",
    "    scores = []\n",
    "    for a in artist_neighbors:\n",
    "        if a in GWiki and GWiki.nodes[a].get(\"sentiment\") is not None:\n",
    "            scores.append(GWiki.nodes[a][\"sentiment\"])\n",
    "        elif a in B and B.nodes[a].get(\"sentiment\") is not None:\n",
    "            scores.append(B.nodes[a][\"sentiment\"])\n",
    "        avg_sentiment = sum(scores) / len(scores) if scores else None\n",
    "\n",
    "    # Save info\n",
    "    community_info.append({\n",
    "        \"index\": i,\n",
    "        \"festivals\": festivals_in_comm,\n",
    "        \"artists\": list(artist_neighbors),\n",
    "        \"avg_sentiment\": avg_sentiment,\n",
    "        \"size\": len(community)\n",
    "    })\n",
    "\n",
    "# Print result\n",
    "print(\"\\nüéº Community Sentiment Overview (via festivals in GWiki):\")\n",
    "for info in community_info:\n",
    "    print(f\"Community {info['index'] + 1}:\")\n",
    "    print(f\"  Size (festivals+artists): {info['size']}\")\n",
    "    print(f\"  Festivals: {', '.join(info['festivals']) if info['festivals'] else 'None'}\")\n",
    "    print(f\"  Artists: {len(info['artists'])}\")\n",
    "    if info['avg_sentiment'] is not None:\n",
    "        print(f\"  Average Sentiment: {info['avg_sentiment']:.3f}\")\n",
    "    else:\n",
    "        print(\"  No sentiment data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ceb10a",
   "metadata": {},
   "source": [
    "### Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4bf6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "comm_docs  = defaultdict(list)   # comm_id -> [texts...]\n",
    "\n",
    "for n in GWiki.nodes():\n",
    "    txt = GWiki.nodes[n].get(\"wikitext\")\n",
    "    if not txt: \n",
    "        continue\n",
    "    txt = clean(txt)\n",
    "\n",
    "    # add to community doc (if in top-K)\n",
    "    cid = node2comm.get(n)\n",
    "    if cid in top_comm_ids:\n",
    "        comm_docs[cid].append(txt)\n",
    "\n",
    "# ---------- concatenate & show sizes ----------\n",
    "\n",
    "for cid, community in enumerate(comms_sorted):\n",
    "    if cid not in top_comm_ids:\n",
    "        continue\n",
    "\n",
    "    # festivals in this community\n",
    "    festivals_in_comm = [n for n in community if G.nodes[n].get(\"bipartite\") == \"festival_year\"]\n",
    "\n",
    "    # artist neighbours from B\n",
    "    artist_neighbors = set()\n",
    "    for f in festivals_in_comm:\n",
    "        if f not in B: \n",
    "            continue\n",
    "        artist_neighbors.update([\n",
    "            nbr for nbr in B.neighbors(f)\n",
    "            if B.nodes[nbr].get(\"bipartite\") == \"artist\"\n",
    "        ])\n",
    "\n",
    "    # collect wikitext from GWiki for those artists\n",
    "    for a in artist_neighbors:\n",
    "        if a in GWiki:\n",
    "            txt = GWiki.nodes[a].get(\"wikitext\")\n",
    "            if txt:\n",
    "                comm_docs[cid].append(clean(txt))\n",
    "\n",
    "# concatenate per community\n",
    "comm_docs = {cid: \" \".join(docs) for cid, docs in comm_docs.items()}\n",
    "\n",
    "def show_top_tfidf(docs_dict, title, top_n=6):\n",
    "    \n",
    "    print(f\"\\nüîπ Top TF‚ÄìIDF words per {title.lower()}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # TF‚ÄìIDF model\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english', # removes commonly used words with little meaning, such as \"the\", \"are\" and \"is\"\n",
    "        lowercase=True,\n",
    "        max_features=5000,\n",
    "        max_df=0.80,\n",
    "        token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    )\n",
    "\n",
    "    labels = list(docs_dict.keys())\n",
    "    texts = [docs_dict[l] for l in labels]\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        row = X[i].toarray().flatten()\n",
    "        top_idx = row.argsort()[-top_n:][::-1]\n",
    "        top_terms = [(terms[j], round(row[j], 3)) for j in top_idx]\n",
    "        print(f\"\\n{title[:-1]} {label}:\")\n",
    "        print(\"   \" + \", \".join([f\"{w} ({v})\" for w, v in top_terms]))\n",
    "\n",
    "\n",
    "# --- Run for top 4 communities ---\n",
    "comm_docs_top10 = {cid: comm_docs[cid] for cid in list(comm_docs.keys())[:6]}\n",
    "show_top_tfidf(comm_docs_top10, \"Communities\", top_n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e32408",
   "metadata": {},
   "source": [
    "### Create Wordclouds\n",
    "Using TF-IDF word clouds are created to visualize the most meaningful words in the documents, for each of the 6 biggest communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccbf2be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_tfidf_wordclouds(docs_dict, title, top_n=100):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        max_features=5000,\n",
    "        max_df=0.80,\n",
    "        token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\"\n",
    "    )\n",
    "    labels = list(docs_dict.keys())\n",
    "    texts = [docs_dict[l] for l in labels]\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # 2x3 grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.flatten()  # flatten to 1D list\n",
    "\n",
    "    for i, label in enumerate(labels[:6]):  # limit to 6 communities\n",
    "        row = X[i].toarray().flatten()\n",
    "        top_idx = row.argsort()[-top_n:]\n",
    "        freqs = {terms[j]: row[j] for j in top_idx}\n",
    "        wc = WordCloud(width=600, height=400, background_color=\"white\").generate_from_frequencies(freqs)\n",
    "        axes[i].imshow(wc, interpolation=\"bilinear\")\n",
    "        axes[i].set_title(f\"{title[:-1]} {label}\", fontsize=12)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    # Hide unused subplots if fewer than 6\n",
    "    for j in range(len(labels), 6):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"TF‚ÄìIDF Word Clouds ‚Äî {title}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Plot for top 6 communities ---\n",
    "comm_docs_top6 = {cid: comm_docs[cid] for cid in list(comm_docs.keys())[:6]}\n",
    "plot_tfidf_wordclouds(comm_docs_top6, \"Communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e116e",
   "metadata": {},
   "source": [
    "## Adding and finding the available Wikipedia pages\n",
    "Before finding the wikipedia pages, the network consisting of the artists and festivals are retrived from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb7d87a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "RAW_URL = \"https://raw.githubusercontent.com/MittaHage/danish-music-festival-ecosystem/main/festival_network_attributes.json\"\n",
    "\n",
    "response = requests.get(RAW_URL, timeout=30)\n",
    "response.raise_for_status()\n",
    "data = response.json()\n",
    "\n",
    "# Build the graph\n",
    "G = nx.node_link_graph(data)   # converts JSON into a NetworkX graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741a688",
   "metadata": {},
   "source": [
    "#### Creating the function for data retrieval\n",
    "When searching for wikipedia pages of the artists, some considerations had to be made. Not all artists are spelled the exact same way in our data as it is on wikipedia, hence some adjustments had to be made to the simple wikitext retrieval code had to be made. \n",
    "\n",
    "An artist such as Gasolin is spelled like Gasolin' on wikipedia, but just when searching gasolin, the wikitext found is a Redirect page, therefore in the cases where the search is redirected, the fetch_data function is run on the title of the redirect, to overcome this obstacle of small differences between our dataset and the titles on wikipedia. However in order to end in an endless loop, a page can only be redirected once. \n",
    "\n",
    "Furthermore some artists have rather general names such as 'Engine' and 'Hair', and therefor the page to be found could be a wiktionary side, and hence if this is the case, it is not the page we would have needed, and hence the text will not be added to the wikitext attribute in the graph.\n",
    "\n",
    "However if a wikipedia page is found, were none of the above is the case, the wikitext is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90f51f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# User-Agent for polite requests \n",
    "UA = \"Mozilla/5.0 (student project)\" \n",
    "OUTDIR = \"Assignment 2 data\" \n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Helper function to fetch page from a given language wiki\n",
    "def fetch_page(title, lang=\"en\", _redirect = False):\n",
    "    baseurl = f\"https://{lang}.wikipedia.org/w/api.php?\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"revisions\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    try:\n",
    "        req = urllib.request.Request(baseurl + urllib.parse.urlencode(params), headers={\"User-Agent\": UA})\n",
    "        with urllib.request.urlopen(req) as response:\n",
    "            data = json.loads(response.read().decode(\"utf-8\"))\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        if \"missing\" in page:\n",
    "            return None\n",
    "        rev = page[\"revisions\"][0]\n",
    "        wikitext = rev.get(\"*\") if \"*\" in rev else rev[\"slots\"][\"main\"][\"*\"]\n",
    "\n",
    "        # If wikitext starts with REDIRECT\n",
    "        if wikitext.upper().startswith(\"#REDIRECT\") and _redirect is False:\n",
    "            # Find redirect m√•l inde i [[...]]\n",
    "            match = re.search(r\"\\[\\[(.*?)\\]\\]\", wikitext)\n",
    "            if match:\n",
    "                redirect_target = match.group(1).strip()\n",
    "                print(f\"‚û°Ô∏è Redirected to: {redirect_target}\")\n",
    "                # K√∏r fetch_page igen p√• redirect_target\n",
    "                return fetch_page(redirect_target.replace(\" \", \"_\").replace(\"-\", \"_\"), lang=lang, _redirect = True) # _redirect = True, to ensure a redirect only occurs once\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # If the page is a wiktionary side, skip it\n",
    "        if wikitext.startswith(\"{{wiktionary\"):\n",
    "            return None\n",
    "        if wikitext.startswith(\"{{Wiktionary\"):\n",
    "            return None\n",
    "        \n",
    "        return wikitext\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching {title} ({lang}): {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec942c",
   "metadata": {},
   "source": [
    "#### Searching for the wikipages\n",
    "As mentioned above, some artists have rather ordinary names as 'engine' and the returned wikipedia page would in this example be a page about an actual engine(like the one in a car), in order to succesfully obtain the desired pages, suffixes such as band and musician is searched for prior to trying just the artist name. \n",
    "\n",
    "Furthermore since the festivals are danish and smaller danish artists might appear, if no english wikipedia is found, a search for a danish wikipedia is created.\n",
    "\n",
    "If a wikipedia page is found, two values are added to their respective attributes in the graph, being the language of the wikipedia page, as well as the wikitext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf74c36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    if node[1].get(\"bipartite\") == \"artist\":\n",
    "        artist_id = node[0]   # node ID\n",
    "        wikitext = None\n",
    "        lang_used = None\n",
    "\n",
    "        en_suffixes = [\"_(musician)\", \"_(band)\", \"_(singer)\", \"_(American_band)\", \"\"]\n",
    "        da_suffixes = [\"_(musiker)\", \"_(band)\", \"_(sanger)\", \"_(kor)\", \"\"]\n",
    "\n",
    "        # Try English Wikipedia\n",
    "        for suffix in en_suffixes:\n",
    "            if wikitext is None:\n",
    "                wikitext = fetch_page(artist_id + suffix, lang=\"en\")\n",
    "                if wikitext:\n",
    "                    lang_used = \"en\"\n",
    "\n",
    "        # Try Danish Wikipedia\n",
    "        if wikitext is None:\n",
    "            for suffix in da_suffixes:\n",
    "                if wikitext is None:\n",
    "                    wikitext = fetch_page(artist_id + suffix, lang=\"da\")\n",
    "                    if wikitext:\n",
    "                        lang_used = \"da\"\n",
    "\n",
    "        # Attach as node attribute\n",
    "        if wikitext:\n",
    "            G.nodes[artist_id][\"wikitext\"] = wikitext\n",
    "            G.nodes[artist_id][\"wiki_language\"] = lang_used\n",
    "        else:\n",
    "            G.nodes[artist_id][\"wikitext\"] = None\n",
    "            G.nodes[artist_id][\"wiki_language\"] = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791053f",
   "metadata": {},
   "source": [
    "### Calculating Sentiment and adding as attribute\n",
    "After adding the text attributes, it is possible to create a sentiment value of the wikitext to the node as well. The sentiment is calculated using the AFINN dataset, since this method exists both in danish or english. \n",
    "\n",
    "When calculating the sentiment the neutral words are excluded, and after calculating the score, it is normalized and the average sentiment value of the wikitext is added as an attribute to the network, such that it can be easily accesed in further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ed8fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "import re\n",
    "\n",
    "afinn_en = Afinn(language='en')\n",
    "afinn_da = Afinn(language='da')\n",
    "\n",
    "def tokenize(text):\n",
    "    if not text:   # catches None or empty string\n",
    "        return []\n",
    "    return re.findall(r'\\b[a-z√¶√∏√•]+\\b', text.lower())\n",
    "\n",
    "for node_id, attrs in G.nodes(data=True):\n",
    "    if attrs.get(\"bipartite\") == \"artist\":\n",
    "        text = attrs.get(\"wikitext\", \"\")\n",
    "        lang = attrs.get(\"wiki_language\", \"en\")\n",
    "\n",
    "        tokens = tokenize(text)\n",
    "\n",
    "        if lang == \"en\":\n",
    "            scores = [afinn_en.score(word) for word in tokens if -1 > afinn_en.score(word) or afinn_en.score(word) > 1]\n",
    "        elif lang == \"da\":\n",
    "            scores = [afinn_da.score(word) for word in tokens if -1 > afinn_en.score(word) or afinn_en.score(word) > 1]\n",
    "        else:\n",
    "            scores = []\n",
    "\n",
    "        if scores:\n",
    "            # Normalize from -5‚Ä¶+5 to 0‚Ä¶10\n",
    "            normalized_scores = [(s + 5) for s in scores]\n",
    "            sentiment_value = sum(normalized_scores) / len(normalized_scores)\n",
    "        else:\n",
    "            sentiment_value = None\n",
    "\n",
    "        # Attach sentiment as a node attribute\n",
    "        G.nodes[node_id][\"sentiment\"] = sentiment_value\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a6ec5",
   "metadata": {},
   "source": [
    "### Saving the graph\n",
    "Saving the graph as a json file locally on computer to upload to GitHub such that it can be retrieved and used by any user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71fa00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert graph to node-link dictionary\n",
    "graph_dict = nx.node_link_data(G)\n",
    "\n",
    "# Choose a specific path on your computer\n",
    "save_path = r\"C:\\Users\\KarolineHeleneBaars√∏\\Desktop\\11 - semester\\Social Graphs\\festival_graph_newWiki.json\"\n",
    "\n",
    "# Save dictionary as JSON file\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(graph_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Graph saved as JSON at {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
